<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Life of an engineer</title>
    <link>https://66mhz.github.io/</link>
    <description>Recent content on Life of an engineer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>66mhz</copyright>
    <lastBuildDate>Sun, 31 Jul 2022 01:22:50 -0500</lastBuildDate><atom:link href="https://66mhz.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SLAM</title>
      <link>https://66mhz.github.io/robotics/slam/</link>
      <pubDate>Sun, 31 Jul 2022 01:22:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/slam/</guid>
      <description>SLAM (Simultaneous Localization and Mapping) In robotics, planning a path is a predictive control method that requires estimation of the structure of environment, to plot a course to a target. SLAM, simultaneous localization and mapping, is the most commonly used method.
SLAM is usually done with a suite of sensors, such as RGB-D (depth) cameras, stereo depth cameras, LIDAR or monocular cameras.
An interesting example is the Skydio R15 drone. It is a quadrotor drone released in 2018, which uses SLAM to build a local map of the environment.</description>
      <content>&lt;h1 id=&#34;slam-simultaneous-localization-and-mapping&#34;&gt;SLAM (Simultaneous Localization and Mapping)&lt;/h1&gt;
&lt;p&gt;In robotics, planning a path is a predictive control method that requires estimation of the structure of environment, to plot a course to a target. SLAM, simultaneous localization and mapping, is the most commonly used method.&lt;/p&gt;
&lt;p&gt;SLAM is usually done with a suite of sensors, such as RGB-D (depth) cameras, stereo depth cameras, LIDAR or monocular cameras.&lt;/p&gt;
&lt;p&gt;An interesting example is the Skydio R15 drone. It is a quadrotor drone released in 2018, which uses SLAM to build a local map of the environment. It is done using 13 cameras, placed to see the whole environment. It uses deep learning and detects the person who it is set to follow and builds a path through the environment in order to follow the person while avoiding obstacles around it.&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;There are some good resources for robot mapping, one of them is &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&#34;&gt;Robot Mapping&lt;/a&gt; course by Wolfram Burgard. Burgard is a professor at the University of Freiburg and head of the research lab for &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/&#34;&gt;Autonomous Intelligent Systems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basis of mapping is visual odometry and there are a few good resources for VO:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[Tutorial on Visual Odometry](Tutorial on Visual Odometry), by Davide Scaramuzza&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rpg.ifi.uzh.ch/teaching.html&#34;&gt;Vision Algorithms for Mobile Robotics&lt;/a&gt;, ETHZ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;—&amp;gt; A good resource for all books, references, and research is provided &lt;a href=&#34;https://github.com/kanster/awesome-slam&#34;&gt;here&lt;/a&gt;, I would recommend taking a look at it.&lt;/p&gt;
&lt;p&gt;—&amp;gt; Another list, rather a practical one (which I prefer) is given &lt;a href=&#34;https://github.com/tzutalin/awesome-visual-slam&#34;&gt;here&lt;/a&gt;, please take a look.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;For software, it is better to stick with (at least at the beginning) well-known libraries. If you are using ROS (Robot Operating System), utilize &lt;a href=&#34;http://wiki.ros.org/rtabmap_ros&#34;&gt;rtabmap_ros&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This package is a ROS wrapper of RTAB-Map (Real-Time Appearance-Based Mapping), a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. The tutorials and demos show some examples of mapping with RTAB-Map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;photogrammetry&#34;&gt;Photogrammetry&lt;/h1&gt;
&lt;h2 id=&#34;learn-1&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;Photogrammetry is “the art and science of extracting 3D information from photographs”. There are many techniques to extract such information.&lt;/p&gt;
&lt;p&gt;One way to do this is, by extracting 3D measurements from 2D. The distance between two points that lie on a plane parallel to the photographic image plane can be determined by measuring their distance on the image, as long as the scale of the image is known. [Projective geometry] (&lt;a href=&#34;https://en.wikipedia.org/wiki/Projective_geometry&#34;&gt;https://en.wikipedia.org/wiki/Projective_geometry&lt;/a&gt;) is the main discipline working on such problems.&lt;/p&gt;
&lt;p&gt;A special case of photogrammetry is stereo-photogrammetry. As stereo implies, there are two pictures taken from different angles, using triangulation it is possible to identify the location of the object.&lt;/p&gt;
&lt;p&gt;Photogrammetric data can also be complemented with LIDAR, laser scanners, or any device that returns point clouds. (For those who don’t know what point cloud is, it is a set of data points in space. Wikipedia has &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif&#34;&gt;a good visual&lt;/a&gt; for it)&lt;/p&gt;
&lt;h3 id=&#34;image-depth-estimation-using-deep-learning&#34;&gt;Image depth estimation using deep learning&lt;/h3&gt;
&lt;p&gt;3D point clouds can also be generated from computer vision algorithms such as triangulation, bundle adjustment, and more recently, monocular image depth estimation using deep learning.&lt;/p&gt;
&lt;p&gt;There are many examples of how to use deep learning to estimate depth from a single image. Here is &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf&#34;&gt;one reference&lt;/a&gt;, which explains the theory well, and also discusses what the models learn.
One of the latest works is &lt;a href=&#34;https://3dvar.com/Bhat2021AdaBins.pdf&#34;&gt;Adabins&lt;/a&gt;. I had a chance to try their implementation and quite frankly it is impressive.
This is a hot research area. Another work from DeepAI is called &lt;a href=&#34;https://deepai.org/publication/localbins-improving-depth-estimation-by-learning-local-distributions&#34;&gt;LocalBins&lt;/a&gt;, which is compared to Adabins. There is a scoreboard list for such algorithms somewhere, &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;The question is how we can use such algorithms in actual robotics implementations and integrate this into a SLAM system. I think that’s something to survey and think about.&lt;/p&gt;
&lt;h3 id=&#34;how-do-neural-networks-see-depth-in-single-images&#34;&gt;How Do Neural Networks See Depth in Single Images?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Stereo vision allows absolute depth to be estimated using multiple cameras. When only a single camera can be used, optical flow can provide a measure of depth; or if images can be combined over longer time spans then SLAM or Structure-from-Motion can be used to estimate the geometry of the scene.  These methods tend to treat depth estimation as a purely geometrical problem, ignoring the content of the images. These methods tend to treat depth estimation as a purely geometrical problem, ignoring the content of the images. \n Methods have been developed that use monocular image sequences to learn single-frame depth estimation in an unsupervised manner, of which the works by Zhou et al. [25] and Wang et al. [23] are examples. Recent work focuses primarily on the accuracy of monocular depth estimation, where evaluations on publicly available datasets such as KITTI [15] and NYUv2 [20] show that neural networks can indeed generate accurate depth maps from single images.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;code-1&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Wikipedia has a good list of comparisons of software, but I have yet to try them.&lt;/p&gt;
&lt;p&gt;If you like drones or work on them, one noticeable software is &lt;a href=&#34;https://www.opendronemap.org/&#34;&gt;OpenDroneMap&lt;/a&gt;. The software has a lot of libraries, which I hope to look into, try and update here soon. TODO.&lt;/p&gt;
&lt;p&gt;Looking for a dataset of point clouds? Here is one —&amp;gt; &lt;a href=&#34;https://pointclouds.org/&#34;&gt;pointclouds.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;PyTorch3D also has functions to render point clouds. One of the tutorials is &lt;a href=&#34;https://pytorch3d.org/tutorials/render_colored_points&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://openresearch-repository.anu.edu.au/bitstream/1885/227241/1/Jean-Luc%20Stevens%20MPhil%20Thesis.pdf&#34;&gt;https://openresearch-repository.anu.edu.au/bitstream/1885/227241/1/Jean-Luc%20Stevens%20MPhil%20Thesis.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&#34;&gt;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/rtabmap_ros&#34;&gt;http://wiki.ros.org/rtabmap_ros&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kanster/awesome-slam&#34;&gt;https://github.com/kanster/awesome-slam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tzutalin/awesome-visual-slam&#34;&gt;https://github.com/tzutalin/awesome-visual-slam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Photogrammetry&#34;&gt;https://en.wikipedia.org/wiki/Photogrammetry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch3d.org/tutorials/render_colored_points&#34;&gt;https://pytorch3d.org/tutorials/render_colored_points&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Knowing Red</title>
      <link>https://66mhz.github.io/random-tech/red/</link>
      <pubDate>Sun, 31 Jul 2022 00:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/random-tech/red/</guid>
      <description>Knowing Red But Not Feeling It [&amp;hellip;] Mary, a colorblind lady, volunteered for an experiment where she was shown the required amount of light to create a red color in her brain. After being shown the light, she affirmed the red color through her description to the neuroscientist in charge. As interesting as the color breakthrough is, it does not fully capture the essence of knowing it. Even though her brain has registered the red color, her mind is yet to do so.</description>
      <content>&lt;h1 id=&#34;knowing-red-but-not-feeling-it&#34;&gt;Knowing Red But Not Feeling It&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] Mary, a colorblind lady, volunteered for an experiment where she was shown the required amount of light to create a red color in her brain. After being shown the light, she affirmed the red color through her description to the neuroscientist in charge. As interesting as the color breakthrough is, it does not fully capture the essence of knowing it. Even though her brain has registered the red color, her mind is yet to do so. She knows what red looks like, but she does not know what red feels like.[&amp;hellip;]&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    </item>
    
    <item>
      <title>TTS Models</title>
      <link>https://66mhz.github.io/ml-dl/texttospeech/</link>
      <pubDate>Wed, 27 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/texttospeech/</guid>
      <description>Text-to-Speech(TTS) Models The ones that are worthy trying:
Larynx It is simple, not the greatest out-of-box, but will work on low-perf devices. Coqui https://github.com/66mhz/TTS </description>
      <content>&lt;h1 id=&#34;text-to-speechtts-models&#34;&gt;Text-to-Speech(TTS) Models&lt;/h1&gt;
&lt;p&gt;The ones that are worthy trying:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/66mhz/larynx#basic-synthesis&#34;&gt;Larynx&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;It is simple, not the greatest out-of-box, but will work on low-perf devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Coqui
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/66mhz/TTS&#34;&gt;https://github.com/66mhz/TTS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>ROS (Robot Operating System)</title>
      <link>https://66mhz.github.io/robotics/ros/</link>
      <pubDate>Sat, 16 Jul 2022 23:23:15 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/ros/</guid>
      <description>Robot Operating System / ROS Their docs has everything you need to refer, so we will just discuss ROS a bit here.
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is AccelerationRobotics. I quote their webpage:
ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient.</description>
      <content>&lt;h1 id=&#34;robot-operating-system--ros&#34;&gt;Robot Operating System / ROS&lt;/h1&gt;
&lt;p&gt;Their &lt;a href=&#34;https://docs.ros.org/&#34;&gt;docs&lt;/a&gt; has everything you need to refer, so we will just discuss ROS a bit here.&lt;br&gt;
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is &lt;a href=&#34;https://accelerationrobotics.com/robotcore.php#benchmarks&#34;&gt;AccelerationRobotics&lt;/a&gt;. I quote their webpage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient. Simply put, it provides a development, build and deployment experience for creating robot hardware and hardware accelerators similar to the standard, non-accelerated ROS development flow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is &lt;a href=&#34;https://arxiv.org/abs/2205.03929&#34;&gt;RobotCore paper&lt;/a&gt; for reference.&lt;/p&gt;
&lt;h1 id=&#34;using-unity-with-ros&#34;&gt;Using Unity with ROS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Endpoint&#34;&gt;ROS TCP Endpoint&lt;/a&gt;&lt;br&gt;
&lt;em&gt;ROS package used to create an endpoint to accept ROS messages sent from a Unity scene using the ROS TCP Connector scripts&lt;/em&gt;\&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Related:&lt;/strong&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Connector&#34;&gt;ROS TCP Connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/Unity-Robotics-Hub&#34;&gt;Robotics simulation using Unity&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/com.unity.perception&#34;&gt;Unity perception package&lt;/a&gt;&lt;br&gt;
&lt;em&gt;The Perception package provides a toolkit for generating large-scale datasets for computer vision training and validation. It is focused on a handful of camera-based use cases for now and will ultimately expand to other forms of sensors and machine learning tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>ROCm</title>
      <link>https://66mhz.github.io/ml-dl/rocm/</link>
      <pubDate>Sat, 16 Jul 2022 23:18:54 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/rocm/</guid>
      <description>For anyone using ROCm for machine learnig / deep learning, these docker run commands will be useful:
For Pytorch:
sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch
Note rocm/pytorch is the latest release, check docker hub for different ubuntu versions and so on.
ROCm with Tensorflow has the similar idea, just use rocm/tensorflow.</description>
      <content>&lt;p&gt;For anyone using ROCm for machine learnig / deep learning, these &lt;code&gt;docker run&lt;/code&gt; commands will be useful:&lt;br&gt;
For &lt;strong&gt;Pytorch&lt;/strong&gt;:&lt;br&gt;
&lt;code&gt;sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch&lt;/code&gt;&lt;br&gt;
Note &lt;code&gt;rocm/pytorch&lt;/code&gt; is the latest release, check &lt;a href=&#34;https://hub.docker.com/r/rocm/pytorch&#34;&gt;docker hub&lt;/a&gt; for different ubuntu versions and so on.&lt;/p&gt;
&lt;p&gt;ROCm with Tensorflow has the similar idea, just use &lt;code&gt;rocm/tensorflow&lt;/code&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Fake Real_Self_Awareness</title>
      <link>https://66mhz.github.io/robotics/fake-real_self_awareness/</link>
      <pubDate>Sat, 16 Jul 2022 23:15:47 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/fake-real_self_awareness/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Agriculture and Robotics</title>
      <link>https://66mhz.github.io/robotics/agriculture_robotics/</link>
      <pubDate>Sat, 16 Jul 2022 22:22:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/agriculture_robotics/</guid>
      <description>Survey TerraSentia (https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html) Functions Measure plant height Stem diameter Leaf-area index Stand count (num of live grain or fruit producing plants) Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.
In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.</description>
      <content>&lt;h1 id=&#34;survey&#34;&gt;Survey&lt;/h1&gt;
&lt;h2 id=&#34;terrasentia&#34;&gt;TerraSentia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&#34;&gt;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;functions&#34;&gt;Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Measure plant height&lt;/li&gt;
&lt;li&gt;Stem diameter&lt;/li&gt;
&lt;li&gt;Leaf-area index&lt;/li&gt;
&lt;li&gt;Stand count (num of live grain or fruit producing plants)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.&lt;/p&gt;
&lt;p&gt;In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>RaspberryPi Camera v2</title>
      <link>https://66mhz.github.io/robotics/raspberrycam/</link>
      <pubDate>Sat, 16 Jul 2022 22:10:03 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/raspberrycam/</guid>
      <description>Raspberry camera version 2.1 Sony IMX219 8-megapixel sensor Sensor: /base/soc/i2c0mux/i2c@1/imx219@10 It can be accessed through the MMAL and V4L APIs. If raspistill command does not work, check libcamera.
To install v4l2-rtspserver, simply use the following command:
sudo snap install v4l2-rtspserver
To check the status of uv4l on raspbian:
sudo service uv4l_raspicam status
To start a stream:
uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on
To find supported video capture formats:</description>
      <content>&lt;ul&gt;
&lt;li&gt;Raspberry camera version 2.1&lt;/li&gt;
&lt;li&gt;Sony IMX219 8-megapixel sensor&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sensor: /base/soc/i2c0mux/i2c@1/imx219@10&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;It can be accessed through the MMAL and V4L APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;code&gt;raspistill&lt;/code&gt; command does not work, check &lt;code&gt;libcamera&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To install &lt;em&gt;v4l2-rtspserver&lt;/em&gt;, simply use the following command:&lt;br&gt;
&lt;code&gt;sudo snap install v4l2-rtspserver&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check the status of uv4l on raspbian:&lt;br&gt;
&lt;code&gt;sudo service uv4l_raspicam status&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To start a stream:&lt;br&gt;
&lt;code&gt;uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To find supported video capture formats:&lt;br&gt;
&lt;code&gt;v4l2-ctl --list-formats&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check supported and detected cameras:&lt;br&gt;
&lt;code&gt;vcgencmd get_camera&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How to use uv4l with a regular USB camera?&lt;br&gt;
&lt;code&gt;uv4l --driver uvc --auto-video_nr --device-path 001:003&lt;/code&gt;
Note &lt;code&gt;001:003&lt;/code&gt; shows the bus location for that USB camera, and we use &lt;code&gt;uvc&lt;/code&gt; for &lt;code&gt;--driver&lt;/code&gt;. This is the module for devices based on the USB video class definition.&lt;/p&gt;
&lt;p&gt;How to initiate stream with uv4l with a regular USB camera using uvc driver?&lt;br&gt;
&lt;code&gt;uv4l --device-path 001:003 --config-file=/etc/uv4l/uv4l-uvc.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;feeding-the-stream-to-slam-and-other-algorithms---disucssion&#34;&gt;Feeding the stream to SLAM and other algorithms - Disucssion&lt;/h3&gt;
&lt;p&gt;One of the approaches we can think of is using &lt;strong&gt;Adabins&lt;/strong&gt; and alike algorithms. These are the algorithms providing you depth information from a single image. The paper itself is quite a good read and &lt;a href=&#34;https://github.com/shariqfarooq123/AdaBins&#34;&gt;the code is here&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>MAIN</title>
      <link>https://66mhz.github.io/posts/main/</link>
      <pubDate>Sat, 16 Jul 2022 20:59:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/posts/main/</guid>
      <description>Select a topic from the navigation bar :)</description>
      <content>&lt;p&gt;Select a topic from the navigation bar :)&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>BLE5</title>
      <link>https://66mhz.github.io/random-tech/ble5/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/random-tech/ble5/</guid>
      <description>In theory, bluetooth uses 2.45GHz and it supports 721KBps of data transfer, along with three voice channels. Standart bluetotth links can connect up to eight devices simultaneously, with each device offering unique 48-bit address based on IEEE 802 standard.
Bluetooth network consists of personal area network or piconet, 2 to 8 devices. One single, 7 masters. Master initiates communication with other devices and governs the link and data traffic between itself and slaves.</description>
      <content>&lt;p&gt;In theory, bluetooth uses 2.45GHz and it supports 721KBps of data transfer, along with three voice channels.
Standart bluetotth links can connect up to &lt;strong&gt;eight devices&lt;/strong&gt; simultaneously, with each device offering unique &lt;em&gt;48-bit&lt;/em&gt; address based on IEEE 802 standard.&lt;/p&gt;
&lt;p&gt;Bluetooth network consists of personal area network or piconet, 2 to 8 devices. One single, 7 masters.
Master initiates communication with other devices and governs the link and data traffic between itself and slaves.&lt;/p&gt;
&lt;p&gt;In 1998, the technology companies Ericsson, IBM, Nokia, and Toshiba formed the Bluetooth Special Interest Group (SIG), which published the first version of the platform in 1999.&lt;br&gt;
This first version could achieve a data transfer rate of &lt;strong&gt;1Mbps&lt;/strong&gt;.&lt;br&gt;
Version 2.0+EDR had a data speed of 3Mbps, while version 3.0+HS stepped up its speed of data transfer to &lt;strong&gt;24 Mbps&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;versions-1-to-3-then-ble&#34;&gt;Versions 1 to 3, then BLE&lt;/h2&gt;
&lt;p&gt;Versions 1 to 3 of the platform operated via Bluetooth radio, which consumes a large amount of energy in its work.&lt;br&gt;
&lt;strong&gt;Bluetooth Low Energy&lt;/strong&gt; technology or BLE was originally created to reduce the power consumption of Bluetooth peripherals.&lt;br&gt;
It was introduced for Bluetooth 4.0 and continued to improve through the BLE4 series, whose last version was 4.2.&lt;/p&gt;
&lt;p&gt;Design and performance-wise, BLE5 has the edge over BLE4, in a number of different aspects.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Speed
&lt;ol&gt;
&lt;li&gt;BLE5 achieves a data transfer speed of &lt;strong&gt;48MBps&lt;/strong&gt;. This is twice that of BLE4. Bluetooth 5.0 has a bandwidth of 5Mbps, which is more than two times that of Bluetooth 4.2, whose maximum bandwidth is 2.1 Mbps. This effectively increases the data rate of BLE5 to 2Mbps, which allows it to reach a net data rate of about 1.4Mbps, if you ignore overheads like addressing. While this isn’t fast enough to stream video, it does permit audio streaming.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Range
&lt;ol&gt;
&lt;li&gt;Range of BLE5 is up to four times that of Bluetooth 4.2. A BLE4 solution can reach a maximum range of about 50m, so with Bluetooth 5.0 something in the vicinity of 200m is possible — though some researchers suggest that BLE5 can be connected up to 300 metres or 985 feet. These figures are for outdoor connections. &lt;strong&gt;Indoors, Bluetooth 5 actively operates within a radius of 40 metres.&lt;/strong&gt; Compare this with the 10m indoor radius of BLE4, and it’s clear that BLE5 has the advantage when it comes to using wireless headphones some distance away from your phone, for example, or for connecting devices throughout a house, as opposed to within a single room.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Broadcast Capability
&lt;ol&gt;
&lt;li&gt;Bluetooth 5 supports data packets eight times bigger than the previous version, with a message capacity of about 255 bytes (BLE4 has a message capacity about 31 bytes). This gives BLE5 considerably more space for its actual data load, and with more data bits in each packet, the net data throughput is also increased. Largely because of the increased range, speed, and message capacity of BLE5, Bluetooth 5 Beacon has been growing in popularity.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compatibility
&lt;ol&gt;
&lt;li&gt;In terms of compatibility, BLE4 works best with devices compatible with version 4 of the series, but will not work with devices that are using Bluetooth 5. BLE5 is backwards compatible with all versions of Bluetooth up to version 4.2 — but with the limitation that not all Bluetooth 5 features may be available on these devices.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Power Consumption
&lt;ol&gt;
&lt;li&gt;While both BLE5 and BLE4 are part of the Bluetooth Low Energy ecosystem, BLE5 has been designed to consume less power than its predecessor. So Bluetooth 5 devices can be left running for longer periods, without putting too much stress on their batteries. Historically, this has been a particular problem with smart watches and devices with smaller form factors, like IoT sensors. With the redesigned power consumption system in Bluetooth 5, most such devices will increase their battery life.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Resiliency
&lt;ol&gt;
&lt;li&gt;BLE5 was developed with the consideration that important processes involving Bluetooth most often occur in an overloaded environment, which negatively affects its operation. Compared to Bluetooth 4.2, BLE5 works much more reliably in overloaded environments.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Security
&lt;ol&gt;
&lt;li&gt;In April 2017, security researchers discovered several exploits in Bluetooth software (collectively called “BlueBorne”) affecting various platforms, including Microsoft Windows, Linux, Apple iOS, and Google’s Android. Some of these exploits could permit an attacker to connect to devices or systems without authentication, and to effectively hijack a complete device. BLE5 has addressed much of this vulnerability, with bit-level security, and authentication controls using a 128bit key.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Memory Formats for Deep Learning</title>
      <link>https://66mhz.github.io/ml-dl/memory_formats_dl/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/memory_formats_dl/</guid>
      <description>Memory Formats for Deep Learning In deep learning, there are certain memory formats we care about. For activation, we care about:
N: Batch C: Channel D: Depth H: Height W: Weight For weights, we care about: G: Groups O: Output channel I: Input channel D: Depth H: Height W: Weight
Why do we care?
It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like?</description>
      <content>&lt;h1 id=&#34;memory-formats-for-deep-learning&#34;&gt;Memory Formats for Deep Learning&lt;/h1&gt;
&lt;p&gt;In deep learning, there are certain memory formats we care about.
For activation, we care about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N: Batch&lt;/li&gt;
&lt;li&gt;C: Channel&lt;/li&gt;
&lt;li&gt;D: Depth&lt;/li&gt;
&lt;li&gt;H: Height&lt;/li&gt;
&lt;li&gt;W: Weight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For weights, we care about:
G: Groups
O: Output channel
I: Input channel
D: Depth
H: Height
W: Weight&lt;/p&gt;
&lt;p&gt;Why do we care?&lt;/p&gt;
&lt;p&gt;It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like? Most of the time you need to know these basics.&lt;/p&gt;
&lt;p&gt;References:
&lt;a href=&#34;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&#34;&gt;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>PHORHUM</title>
      <link>https://66mhz.github.io/ml-dl/phorhum/</link>
      <pubDate>Mon, 11 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/phorhum/</guid>
      <description>A new work from Google Research, published here.
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. Where is the model? Where is the dataset? We would like to try Google!
If you happen to know the source code or dataset for this work, please share :)
In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.
It can reconstruct full 3D geometry, meaning, even the unseen regions.</description>
      <content>&lt;p&gt;A new work from Google Research, published &lt;a href=&#34;https://phorhum.github.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing.&lt;/strong&gt;
&lt;em&gt;Where is the model? Where is the dataset? We would like to try Google!&lt;/em&gt;&lt;br&gt;
If you happen to know the source code or dataset for this work, please share :)&lt;/p&gt;
&lt;p&gt;In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.&lt;br&gt;
It can reconstruct full 3D geometry, meaning, even the unseen regions. This is the interesting part of the work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://66mhz.github.io/phorhum_pic.gif&#34; alt=&#34;Scenario 1: Across columns&#34;&gt;&lt;/p&gt;
&lt;p&gt;“Rigging” is the term for finding the joints or points for animation.&lt;br&gt;
That reminds me, there is an interesting paper/work for constructing rigs from a given 3D input. It is called &lt;a href=&#34;https://arxiv.org/pdf/2005.00559.pdf&#34;&gt;RigNet&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title></title>
      <link>https://66mhz.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://66mhz.github.io/about/about/</guid>
      <description>this is a test for about page. </description>
      <content>&lt;h2 id=&#34;this-is-a-test-for-about-page&#34;&gt;this is a test for about page.&lt;/h2&gt;
</content>
    </item>
    
    <item>
      <title></title>
      <link>https://66mhz.github.io/computer_architecture/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://66mhz.github.io/computer_architecture/about/</guid>
      <description>this is a test for comp arch </description>
      <content>&lt;h2 id=&#34;this-is-a-test-for-comp-arch&#34;&gt;this is a test for comp arch&lt;/h2&gt;
</content>
    </item>
    
  </channel>
</rss>
