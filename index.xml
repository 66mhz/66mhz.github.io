<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Life of an engineer</title>
    <link>https://66mhz.github.io/</link>
    <description>Recent content on Life of an engineer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>66mhz</copyright>
    <lastBuildDate>Sat, 16 Jul 2022 23:23:15 -0500</lastBuildDate><atom:link href="https://66mhz.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ros</title>
      <link>https://66mhz.github.io/robotics/ros/</link>
      <pubDate>Sat, 16 Jul 2022 23:23:15 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/ros/</guid>
      <description>Robot Operating System / ROS Their docs has everything you need to refer, so we will just discuss ROS a bit here.
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is AccelerationRobotics. I quote their webpage:
ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient.</description>
      <content>&lt;h1 id=&#34;robot-operating-system--ros&#34;&gt;Robot Operating System / ROS&lt;/h1&gt;
&lt;p&gt;Their &lt;a href=&#34;https://docs.ros.org/&#34;&gt;docs&lt;/a&gt; has everything you need to refer, so we will just discuss ROS a bit here.&lt;br&gt;
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is &lt;a href=&#34;https://accelerationrobotics.com/robotcore.php#benchmarks&#34;&gt;AccelerationRobotics&lt;/a&gt;. I quote their webpage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient. Simply put, it provides a development, build and deployment experience for creating robot hardware and hardware accelerators similar to the standard, non-accelerated ROS development flow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is &lt;a href=&#34;https://arxiv.org/abs/2205.03929&#34;&gt;RobotCore paper&lt;/a&gt; for reference.&lt;/p&gt;
&lt;h1 id=&#34;using-unity-with-ros&#34;&gt;Using Unity with ROS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Endpoint&#34;&gt;ROS TCP Endpoint&lt;/a&gt;&lt;br&gt;
&lt;em&gt;ROS package used to create an endpoint to accept ROS messages sent from a Unity scene using the ROS TCP Connector scripts&lt;/em&gt;\&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Related:&lt;/strong&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Connector&#34;&gt;ROS TCP Connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/Unity-Robotics-Hub&#34;&gt;Robotics simulation using Unity&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/com.unity.perception&#34;&gt;Unity perception package&lt;/a&gt;&lt;br&gt;
&lt;em&gt;The Perception package provides a toolkit for generating large-scale datasets for computer vision training and validation. It is focused on a handful of camera-based use cases for now and will ultimately expand to other forms of sensors and machine learning tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>ROCm</title>
      <link>https://66mhz.github.io/ml-dl/rocm/</link>
      <pubDate>Sat, 16 Jul 2022 23:18:54 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/rocm/</guid>
      <description>For anyone using ROCm for machine learnig / deep learning, these docker run commands will be useful:
For Pytorch:
sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch
Note rocm/pytorch is the latest release, check docker hub for different ubuntu versions and so on.
ROCm with Tensorflow has the similar idea, just use rocm/tensorflow.</description>
      <content>&lt;p&gt;For anyone using ROCm for machine learnig / deep learning, these &lt;code&gt;docker run&lt;/code&gt; commands will be useful:&lt;br&gt;
For &lt;strong&gt;Pytorch&lt;/strong&gt;:&lt;br&gt;
&lt;code&gt;sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch&lt;/code&gt;&lt;br&gt;
Note &lt;code&gt;rocm/pytorch&lt;/code&gt; is the latest release, check &lt;a href=&#34;https://hub.docker.com/r/rocm/pytorch&#34;&gt;docker hub&lt;/a&gt; for different ubuntu versions and so on.&lt;/p&gt;
&lt;p&gt;ROCm with Tensorflow has the similar idea, just use &lt;code&gt;rocm/tensorflow&lt;/code&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Fake Real_Self_Awareness</title>
      <link>https://66mhz.github.io/robotics/fake-real_self_awareness/</link>
      <pubDate>Sat, 16 Jul 2022 23:15:47 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/fake-real_self_awareness/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Agriculture and Robotics</title>
      <link>https://66mhz.github.io/robotics/agriculture_robotics/</link>
      <pubDate>Sat, 16 Jul 2022 22:22:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/agriculture_robotics/</guid>
      <description>Survey TerraSentia (https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html) Functions Measure plant height Stem diameter Leaf-area index Stand count (num of live grain or fruit producing plants) Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.
In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.</description>
      <content>&lt;h1 id=&#34;survey&#34;&gt;Survey&lt;/h1&gt;
&lt;h2 id=&#34;terrasentia&#34;&gt;TerraSentia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&#34;&gt;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;functions&#34;&gt;Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Measure plant height&lt;/li&gt;
&lt;li&gt;Stem diameter&lt;/li&gt;
&lt;li&gt;Leaf-area index&lt;/li&gt;
&lt;li&gt;Stand count (num of live grain or fruit producing plants)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.&lt;/p&gt;
&lt;p&gt;In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>RaspberryPi Camera v2</title>
      <link>https://66mhz.github.io/robotics/raspberrycam/</link>
      <pubDate>Sat, 16 Jul 2022 22:10:03 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/raspberrycam/</guid>
      <description>Raspberry camera version 2.1 Sony IMX219 8-megapixel sensor Sensor: /base/soc/i2c0mux/i2c@1/imx219@10 It can be accessed through the MMAL and V4L APIs. If raspistill command does not work, check libcamera.
To install v4l2-rtspserver, simply use the following command:
sudo snap install v4l2-rtspserver
To check the status of uv4l on raspbian:
sudo service uv4l_raspicam status
To start a stream:
uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on
To find supported video capture formats:</description>
      <content>&lt;ul&gt;
&lt;li&gt;Raspberry camera version 2.1&lt;/li&gt;
&lt;li&gt;Sony IMX219 8-megapixel sensor&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sensor: /base/soc/i2c0mux/i2c@1/imx219@10&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;It can be accessed through the MMAL and V4L APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;code&gt;raspistill&lt;/code&gt; command does not work, check &lt;code&gt;libcamera&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To install &lt;em&gt;v4l2-rtspserver&lt;/em&gt;, simply use the following command:&lt;br&gt;
&lt;code&gt;sudo snap install v4l2-rtspserver&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check the status of uv4l on raspbian:&lt;br&gt;
&lt;code&gt;sudo service uv4l_raspicam status&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To start a stream:&lt;br&gt;
&lt;code&gt;uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To find supported video capture formats:&lt;br&gt;
&lt;code&gt;v4l2-ctl --list-formats&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check supported and detected cameras:&lt;br&gt;
&lt;code&gt;vcgencmd get_camera&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How to use uv4l with a regular USB camera?&lt;br&gt;
&lt;code&gt;uv4l --driver uvc --auto-video_nr --device-path 001:003&lt;/code&gt;
Note &lt;code&gt;001:003&lt;/code&gt; shows the bus location for that USB camera, and we use &lt;code&gt;uvc&lt;/code&gt; for &lt;code&gt;--driver&lt;/code&gt;. This is the module for devices based on the USB video class definition.&lt;/p&gt;
&lt;p&gt;How to initiate stream with uv4l with a regular USB camera using uvc driver?&lt;br&gt;
&lt;code&gt;uv4l --device-path 001:003 --config-file=/etc/uv4l/uv4l-uvc.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;feeding-the-stream-to-slam-and-other-algorithms---disucssion&#34;&gt;Feeding the stream to SLAM and other algorithms - Disucssion&lt;/h3&gt;
&lt;p&gt;One of the approaches we can think of is using &lt;strong&gt;Adabins&lt;/strong&gt; and alike algorithms. These are the algorithms providing you depth information from a single image. The paper itself is quite a good read and &lt;a href=&#34;https://github.com/shariqfarooq123/AdaBins&#34;&gt;the code is here&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>System Design</title>
      <link>https://66mhz.github.io/random-tech/system_design/</link>
      <pubDate>Sat, 16 Jul 2022 21:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/random-tech/system_design/</guid>
      <description>System design is about understanding the fundementals of building blocks, and piecing them together using problem-solving skills. It is about showing leadership skills, showing initiativeness and discussing the trade-offs, as there is often a trade-off once a decision is made. Rather than the solution, how you approach to a problem, to show problem-solving skills is what matters. It is not to memorize solutions. It is about how to solve problems.</description>
      <content>&lt;ul&gt;
&lt;li&gt;System design is about understanding the fundementals of building blocks, and piecing them together using problem-solving skills.&lt;/li&gt;
&lt;li&gt;It is about showing leadership skills, showing initiativeness and discussing the trade-offs, as there is often a trade-off once a decision is made.&lt;/li&gt;
&lt;li&gt;Rather than the solution, how you approach to a problem, to show problem-solving skills is what matters.&lt;/li&gt;
&lt;li&gt;It is not to memorize solutions. It is about how to solve problems. It is about:
&lt;ul&gt;
&lt;li&gt;Understand the requirements&lt;/li&gt;
&lt;li&gt;Find solutions&lt;/li&gt;
&lt;li&gt;Make decisions and discuss trade-offs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Schedule a live-session mock interviews with senior engineers.&lt;/li&gt;
&lt;li&gt;There is a typical flow to it.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gather requirements.&lt;/strong&gt; Clarify the requirements, establish baseline and assumptions made.
&lt;ul&gt;
&lt;li&gt;Who is this for? What are the features we need?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Design API
&lt;ul&gt;
&lt;li&gt;Do QPS if needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Draw high level diagrams&lt;/li&gt;
&lt;li&gt;Schema and data structures
&lt;ul&gt;
&lt;li&gt;Optional storage calculation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;End to end flow summary&lt;/li&gt;
&lt;li&gt;Maybe some deep dives&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>MAIN</title>
      <link>https://66mhz.github.io/posts/main/</link>
      <pubDate>Sat, 16 Jul 2022 20:59:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/posts/main/</guid>
      <description>Select a topic from the navigation bar :)</description>
      <content>&lt;p&gt;Select a topic from the navigation bar :)&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>BLE5</title>
      <link>https://66mhz.github.io/random-tech/ble5/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/random-tech/ble5/</guid>
      <description>In theory, bluetooth uses 2.45GHz and it supports 721KBps of data transfer, along with three voice channels. Standart bluetotth links can connect up to eight devices simultaneously, with each device offering unique 48-bit address based on IEEE 802 standard.
Bluetooth network consists of personal area network or piconet, 2 to 8 devices. One single, 7 masters. Master initiates communication with other devices and governs the link and data traffic between itself and slaves.</description>
      <content>&lt;p&gt;In theory, bluetooth uses 2.45GHz and it supports 721KBps of data transfer, along with three voice channels.
Standart bluetotth links can connect up to &lt;strong&gt;eight devices&lt;/strong&gt; simultaneously, with each device offering unique &lt;em&gt;48-bit&lt;/em&gt; address based on IEEE 802 standard.&lt;/p&gt;
&lt;p&gt;Bluetooth network consists of personal area network or piconet, 2 to 8 devices. One single, 7 masters.
Master initiates communication with other devices and governs the link and data traffic between itself and slaves.&lt;/p&gt;
&lt;p&gt;In 1998, the technology companies Ericsson, IBM, Nokia, and Toshiba formed the Bluetooth Special Interest Group (SIG), which published the first version of the platform in 1999.&lt;br&gt;
This first version could achieve a data transfer rate of &lt;strong&gt;1Mbps&lt;/strong&gt;.&lt;br&gt;
Version 2.0+EDR had a data speed of 3Mbps, while version 3.0+HS stepped up its speed of data transfer to &lt;strong&gt;24 Mbps&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;versions-1-to-3-then-ble&#34;&gt;Versions 1 to 3, then BLE&lt;/h2&gt;
&lt;p&gt;Versions 1 to 3 of the platform operated via Bluetooth radio, which consumes a large amount of energy in its work.&lt;br&gt;
&lt;strong&gt;Bluetooth Low Energy&lt;/strong&gt; technology or BLE was originally created to reduce the power consumption of Bluetooth peripherals.&lt;br&gt;
It was introduced for Bluetooth 4.0 and continued to improve through the BLE4 series, whose last version was 4.2.&lt;/p&gt;
&lt;p&gt;Design and performance-wise, BLE5 has the edge over BLE4, in a number of different aspects.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Speed
&lt;ol&gt;
&lt;li&gt;BLE5 achieves a data transfer speed of &lt;strong&gt;48MBps&lt;/strong&gt;. This is twice that of BLE4. Bluetooth 5.0 has a bandwidth of 5Mbps, which is more than two times that of Bluetooth 4.2, whose maximum bandwidth is 2.1 Mbps. This effectively increases the data rate of BLE5 to 2Mbps, which allows it to reach a net data rate of about 1.4Mbps, if you ignore overheads like addressing. While this isn’t fast enough to stream video, it does permit audio streaming.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Range
&lt;ol&gt;
&lt;li&gt;Range of BLE5 is up to four times that of Bluetooth 4.2. A BLE4 solution can reach a maximum range of about 50m, so with Bluetooth 5.0 something in the vicinity of 200m is possible — though some researchers suggest that BLE5 can be connected up to 300 metres or 985 feet. These figures are for outdoor connections. &lt;strong&gt;Indoors, Bluetooth 5 actively operates within a radius of 40 metres.&lt;/strong&gt; Compare this with the 10m indoor radius of BLE4, and it’s clear that BLE5 has the advantage when it comes to using wireless headphones some distance away from your phone, for example, or for connecting devices throughout a house, as opposed to within a single room.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Broadcast Capability
&lt;ol&gt;
&lt;li&gt;Bluetooth 5 supports data packets eight times bigger than the previous version, with a message capacity of about 255 bytes (BLE4 has a message capacity about 31 bytes). This gives BLE5 considerably more space for its actual data load, and with more data bits in each packet, the net data throughput is also increased. Largely because of the increased range, speed, and message capacity of BLE5, Bluetooth 5 Beacon has been growing in popularity.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compatibility
&lt;ol&gt;
&lt;li&gt;In terms of compatibility, BLE4 works best with devices compatible with version 4 of the series, but will not work with devices that are using Bluetooth 5. BLE5 is backwards compatible with all versions of Bluetooth up to version 4.2 — but with the limitation that not all Bluetooth 5 features may be available on these devices.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Power Consumption
&lt;ol&gt;
&lt;li&gt;While both BLE5 and BLE4 are part of the Bluetooth Low Energy ecosystem, BLE5 has been designed to consume less power than its predecessor. So Bluetooth 5 devices can be left running for longer periods, without putting too much stress on their batteries. Historically, this has been a particular problem with smart watches and devices with smaller form factors, like IoT sensors. With the redesigned power consumption system in Bluetooth 5, most such devices will increase their battery life.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Resiliency
&lt;ol&gt;
&lt;li&gt;BLE5 was developed with the consideration that important processes involving Bluetooth most often occur in an overloaded environment, which negatively affects its operation. Compared to Bluetooth 4.2, BLE5 works much more reliably in overloaded environments.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Security
&lt;ol&gt;
&lt;li&gt;In April 2017, security researchers discovered several exploits in Bluetooth software (collectively called “BlueBorne”) affecting various platforms, including Microsoft Windows, Linux, Apple iOS, and Google’s Android. Some of these exploits could permit an attacker to connect to devices or systems without authentication, and to effectively hijack a complete device. BLE5 has addressed much of this vulnerability, with bit-level security, and authentication controls using a 128bit key.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Memory Formats for Deep Learning</title>
      <link>https://66mhz.github.io/ml-dl/memory_formats_dl/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/memory_formats_dl/</guid>
      <description>Memory Formats for Deep Learning In deep learning, there are certain memory formats we care about. For activation, we care about:
N: Batch C: Channel D: Depth H: Height W: Weight For weights, we care about: G: Groups O: Output channel I: Input channel D: Depth H: Height W: Weight
Why do we care?
It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like?</description>
      <content>&lt;h1 id=&#34;memory-formats-for-deep-learning&#34;&gt;Memory Formats for Deep Learning&lt;/h1&gt;
&lt;p&gt;In deep learning, there are certain memory formats we care about.
For activation, we care about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N: Batch&lt;/li&gt;
&lt;li&gt;C: Channel&lt;/li&gt;
&lt;li&gt;D: Depth&lt;/li&gt;
&lt;li&gt;H: Height&lt;/li&gt;
&lt;li&gt;W: Weight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For weights, we care about:
G: Groups
O: Output channel
I: Input channel
D: Depth
H: Height
W: Weight&lt;/p&gt;
&lt;p&gt;Why do we care?&lt;/p&gt;
&lt;p&gt;It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like? Most of the time you need to know these basics.&lt;/p&gt;
&lt;p&gt;References:
&lt;a href=&#34;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&#34;&gt;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>PHORHUM</title>
      <link>https://66mhz.github.io/ml-dl/phorhum/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/phorhum/</guid>
      <description>A new work from Google Research, published here.
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. Where is the model? Where is the dataset? We would like to try Google!
If you happen to know the source code or dataset for this work, please share :)
In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.
It can reconstruct full 3D geometry, meaning, even the unseen regions.</description>
      <content>&lt;p&gt;A new work from Google Research, published &lt;a href=&#34;https://phorhum.github.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing.&lt;/strong&gt;
&lt;em&gt;Where is the model? Where is the dataset? We would like to try Google!&lt;/em&gt;&lt;br&gt;
If you happen to know the source code or dataset for this work, please share :)&lt;/p&gt;
&lt;p&gt;In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.&lt;br&gt;
It can reconstruct full 3D geometry, meaning, even the unseen regions. This is the interesting part of the work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://66mhz.github.io/phorhum_pic.gif&#34; alt=&#34;Scenario 1: Across columns&#34;&gt;&lt;/p&gt;
&lt;p&gt;“Rigging” is the term for finding the joints or points for animation.&lt;br&gt;
That reminds me, there is an interesting paper/work for constructing rigs from a given 3D input. It is called &lt;a href=&#34;https://arxiv.org/pdf/2005.00559.pdf&#34;&gt;RigNet&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title></title>
      <link>https://66mhz.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://66mhz.github.io/about/about/</guid>
      <description>this is a test for about page. </description>
      <content>&lt;h2 id=&#34;this-is-a-test-for-about-page&#34;&gt;this is a test for about page.&lt;/h2&gt;
</content>
    </item>
    
    <item>
      <title></title>
      <link>https://66mhz.github.io/computer_architecture/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://66mhz.github.io/computer_architecture/about/</guid>
      <description>this is a test for comp arch </description>
      <content>&lt;h2 id=&#34;this-is-a-test-for-comp-arch&#34;&gt;this is a test for comp arch&lt;/h2&gt;
</content>
    </item>
    
  </channel>
</rss>
