<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML-DLs on Life of an engineer</title>
    <link>https://66mhz.github.io/ml-dl/</link>
    <description>Recent content in ML-DLs on Life of an engineer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>66mhz</copyright>
    <lastBuildDate>Sat, 16 Jul 2022 23:18:54 -0500</lastBuildDate><atom:link href="https://66mhz.github.io/ml-dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ROCm</title>
      <link>https://66mhz.github.io/ml-dl/rocm/</link>
      <pubDate>Sat, 16 Jul 2022 23:18:54 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/rocm/</guid>
      <description>For anyone using ROCm for machine learnig / deep learning, these docker run commands will be useful:
For Pytorch:
sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch
Note rocm/pytorch is the latest release, check docker hub for different ubuntu versions and so on.
ROCm with Tensorflow has the similar idea, just use rocm/tensorflow.</description>
      <content>&lt;p&gt;For anyone using ROCm for machine learnig / deep learning, these &lt;code&gt;docker run&lt;/code&gt; commands will be useful:&lt;br&gt;
For &lt;strong&gt;Pytorch&lt;/strong&gt;:&lt;br&gt;
&lt;code&gt;sudo docker run -it --name rocm_torch --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v ~/things/:/atdocker -v /mnt/1tb:/1tb rocm/pytorch&lt;/code&gt;&lt;br&gt;
Note &lt;code&gt;rocm/pytorch&lt;/code&gt; is the latest release, check &lt;a href=&#34;https://hub.docker.com/r/rocm/pytorch&#34;&gt;docker hub&lt;/a&gt; for different ubuntu versions and so on.&lt;/p&gt;
&lt;p&gt;ROCm with Tensorflow has the similar idea, just use &lt;code&gt;rocm/tensorflow&lt;/code&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Memory Formats for Deep Learning</title>
      <link>https://66mhz.github.io/ml-dl/memory_formats_dl/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/memory_formats_dl/</guid>
      <description>Memory Formats for Deep Learning In deep learning, there are certain memory formats we care about. For activation, we care about:
N: Batch C: Channel D: Depth H: Height W: Weight For weights, we care about: G: Groups O: Output channel I: Input channel D: Depth H: Height W: Weight
Why do we care?
It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like?</description>
      <content>&lt;h1 id=&#34;memory-formats-for-deep-learning&#34;&gt;Memory Formats for Deep Learning&lt;/h1&gt;
&lt;p&gt;In deep learning, there are certain memory formats we care about.
For activation, we care about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N: Batch&lt;/li&gt;
&lt;li&gt;C: Channel&lt;/li&gt;
&lt;li&gt;D: Depth&lt;/li&gt;
&lt;li&gt;H: Height&lt;/li&gt;
&lt;li&gt;W: Weight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For weights, we care about:
G: Groups
O: Output channel
I: Input channel
D: Depth
H: Height
W: Weight&lt;/p&gt;
&lt;p&gt;Why do we care?&lt;/p&gt;
&lt;p&gt;It is mostly to optimize your HW and SW together. Given a HW how do you write your SW, or given SW requirements how a HW design should look like? Most of the time you need to know these basics.&lt;/p&gt;
&lt;p&gt;References:
&lt;a href=&#34;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&#34;&gt;https://oneapi-src.github.io/oneDNN/understanding_memory_formats.html&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>PHORHUM</title>
      <link>https://66mhz.github.io/ml-dl/phorhum/</link>
      <pubDate>Sat, 16 Jul 2022 20:29:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/ml-dl/phorhum/</guid>
      <description>A new work from Google Research, published here.
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. Where is the model? Where is the dataset? We would like to try Google!
If you happen to know the source code or dataset for this work, please share :)
In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.
It can reconstruct full 3D geometry, meaning, even the unseen regions.</description>
      <content>&lt;p&gt;A new work from Google Research, published &lt;a href=&#34;https://phorhum.github.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing.&lt;/strong&gt;
&lt;em&gt;Where is the model? Where is the dataset? We would like to try Google!&lt;/em&gt;&lt;br&gt;
If you happen to know the source code or dataset for this work, please share :)&lt;/p&gt;
&lt;p&gt;In essence, a DL network that performs photorealistic 3D person reconstruction from a single RGB image.&lt;br&gt;
It can reconstruct full 3D geometry, meaning, even the unseen regions. This is the interesting part of the work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://66mhz.github.io/phorhum_pic.gif&#34; alt=&#34;Scenario 1: Across columns&#34;&gt;&lt;/p&gt;
&lt;p&gt;“Rigging” is the term for finding the joints or points for animation.&lt;br&gt;
That reminds me, there is an interesting paper/work for constructing rigs from a given 3D input. It is called &lt;a href=&#34;https://arxiv.org/pdf/2005.00559.pdf&#34;&gt;RigNet&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
