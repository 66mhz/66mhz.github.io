<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics on Life of an engineer</title>
    <link>https://66mhz.github.io/robotics/</link>
    <description>Recent content in Robotics on Life of an engineer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>66mhz</copyright>
    <lastBuildDate>Sun, 31 Jul 2022 01:22:50 -0500</lastBuildDate><atom:link href="https://66mhz.github.io/robotics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SLAM</title>
      <link>https://66mhz.github.io/robotics/slam/</link>
      <pubDate>Sun, 31 Jul 2022 01:22:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/slam/</guid>
      <description>SLAM (Simultaneous Localization and Mapping) In robotics, planning a path is a predictive control method that requires estimation of the structure of environment, to plot a course to a target. SLAM, simultaneous localization and mapping, is the most commonly used method.
SLAM is usually done with a suite of sensors, such as RGB-D (depth) cameras, stereo depth cameras, LIDAR or monocular cameras.
An interesting example is the Skydio R15 drone. It is a quadrotor drone released in 2018, which uses SLAM to build a local map of the environment.</description>
      <content>&lt;h1 id=&#34;slam-simultaneous-localization-and-mapping&#34;&gt;SLAM (Simultaneous Localization and Mapping)&lt;/h1&gt;
&lt;p&gt;In robotics, planning a path is a predictive control method that requires estimation of the structure of environment, to plot a course to a target. SLAM, simultaneous localization and mapping, is the most commonly used method.&lt;/p&gt;
&lt;p&gt;SLAM is usually done with a suite of sensors, such as RGB-D (depth) cameras, stereo depth cameras, LIDAR or monocular cameras.&lt;/p&gt;
&lt;p&gt;An interesting example is the Skydio R15 drone. It is a quadrotor drone released in 2018, which uses SLAM to build a local map of the environment. It is done using 13 cameras, placed to see the whole environment. It uses deep learning and detects the person who it is set to follow and builds a path through the environment in order to follow the person while avoiding obstacles around it.&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;There are some good resources for robot mapping, one of them is &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&#34;&gt;Robot Mapping&lt;/a&gt; course by Wolfram Burgard. Burgard is a professor at the University of Freiburg and head of the research lab for &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/&#34;&gt;Autonomous Intelligent Systems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basis of mapping is visual odometry and there are a few good resources for VO:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[Tutorial on Visual Odometry](Tutorial on Visual Odometry), by Davide Scaramuzza&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rpg.ifi.uzh.ch/teaching.html&#34;&gt;Vision Algorithms for Mobile Robotics&lt;/a&gt;, ETHZ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;—&amp;gt; A good resource for all books, references, and research is provided &lt;a href=&#34;https://github.com/kanster/awesome-slam&#34;&gt;here&lt;/a&gt;, I would recommend taking a look at it.&lt;/p&gt;
&lt;p&gt;—&amp;gt; Another list, rather a practical one (which I prefer) is given &lt;a href=&#34;https://github.com/tzutalin/awesome-visual-slam&#34;&gt;here&lt;/a&gt;, please take a look.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;For software, it is better to stick with (at least at the beginning) well-known libraries. If you are using ROS (Robot Operating System), utilize &lt;a href=&#34;http://wiki.ros.org/rtabmap_ros&#34;&gt;rtabmap_ros&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This package is a ROS wrapper of RTAB-Map (Real-Time Appearance-Based Mapping), a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. The tutorials and demos show some examples of mapping with RTAB-Map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;photogrammetry&#34;&gt;Photogrammetry&lt;/h1&gt;
&lt;h2 id=&#34;learn-1&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;Photogrammetry is “the art and science of extracting 3D information from photographs”. There are many techniques to extract such information.&lt;/p&gt;
&lt;p&gt;One way to do this is, by extracting 3D measurements from 2D. The distance between two points that lie on a plane parallel to the photographic image plane can be determined by measuring their distance on the image, as long as the scale of the image is known. [Projective geometry] (&lt;a href=&#34;https://en.wikipedia.org/wiki/Projective_geometry&#34;&gt;https://en.wikipedia.org/wiki/Projective_geometry&lt;/a&gt;) is the main discipline working on such problems.&lt;/p&gt;
&lt;p&gt;A special case of photogrammetry is stereo-photogrammetry. As stereo implies, there are two pictures taken from different angles, using triangulation it is possible to identify the location of the object.&lt;/p&gt;
&lt;p&gt;Photogrammetric data can also be complemented with LIDAR, laser scanners, or any device that returns point clouds. (For those who don’t know what point cloud is, it is a set of data points in space. Wikipedia has &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif&#34;&gt;a good visual&lt;/a&gt; for it)&lt;/p&gt;
&lt;h3 id=&#34;image-depth-estimation-using-deep-learning&#34;&gt;Image depth estimation using deep learning&lt;/h3&gt;
&lt;p&gt;3D point clouds can also be generated from computer vision algorithms such as triangulation, bundle adjustment, and more recently, monocular image depth estimation using deep learning.&lt;/p&gt;
&lt;p&gt;There are many examples of how to use deep learning to estimate depth from a single image. Here is &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf&#34;&gt;one reference&lt;/a&gt;, which explains the theory well, and also discusses what the models learn.
One of the latest works is &lt;a href=&#34;https://3dvar.com/Bhat2021AdaBins.pdf&#34;&gt;Adabins&lt;/a&gt;. I had a chance to try their implementation and quite frankly it is impressive.
This is a hot research area. Another work from DeepAI is called &lt;a href=&#34;https://deepai.org/publication/localbins-improving-depth-estimation-by-learning-local-distributions&#34;&gt;LocalBins&lt;/a&gt;, which is compared to Adabins. There is a scoreboard list for such algorithms somewhere, &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;The question is how we can use such algorithms in actual robotics implementations and integrate this into a SLAM system. I think that’s something to survey and think about.&lt;/p&gt;
&lt;h3 id=&#34;how-do-neural-networks-see-depth-in-single-images&#34;&gt;How Do Neural Networks See Depth in Single Images?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Stereo vision allows absolute depth to be estimated using multiple cameras. When only a single camera can be used, optical flow can provide a measure of depth; or if images can be combined over longer time spans then SLAM or Structure-from-Motion can be used to estimate the geometry of the scene.  These methods tend to treat depth estimation as a purely geometrical problem, ignoring the content of the images. These methods tend to treat depth estimation as a purely geometrical problem, ignoring the content of the images. \n Methods have been developed that use monocular image sequences to learn single-frame depth estimation in an unsupervised manner, of which the works by Zhou et al. [25] and Wang et al. [23] are examples. Recent work focuses primarily on the accuracy of monocular depth estimation, where evaluations on publicly available datasets such as KITTI [15] and NYUv2 [20] show that neural networks can indeed generate accurate depth maps from single images.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;code-1&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Wikipedia has a good list of comparisons of software, but I have yet to try them.&lt;/p&gt;
&lt;p&gt;If you like drones or work on them, one noticeable software is &lt;a href=&#34;https://www.opendronemap.org/&#34;&gt;OpenDroneMap&lt;/a&gt;. The software has a lot of libraries, which I hope to look into, try and update here soon. TODO.&lt;/p&gt;
&lt;p&gt;Looking for a dataset of point clouds? Here is one —&amp;gt; &lt;a href=&#34;https://pointclouds.org/&#34;&gt;pointclouds.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;PyTorch3D also has functions to render point clouds. One of the tutorials is &lt;a href=&#34;https://pytorch3d.org/tutorials/render_colored_points&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://openresearch-repository.anu.edu.au/bitstream/1885/227241/1/Jean-Luc%20Stevens%20MPhil%20Thesis.pdf&#34;&gt;https://openresearch-repository.anu.edu.au/bitstream/1885/227241/1/Jean-Luc%20Stevens%20MPhil%20Thesis.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&#34;&gt;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/rtabmap_ros&#34;&gt;http://wiki.ros.org/rtabmap_ros&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kanster/awesome-slam&#34;&gt;https://github.com/kanster/awesome-slam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tzutalin/awesome-visual-slam&#34;&gt;https://github.com/tzutalin/awesome-visual-slam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Photogrammetry&#34;&gt;https://en.wikipedia.org/wiki/Photogrammetry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch3d.org/tutorials/render_colored_points&#34;&gt;https://pytorch3d.org/tutorials/render_colored_points&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>ROS (Robot Operating System)</title>
      <link>https://66mhz.github.io/robotics/ros/</link>
      <pubDate>Sat, 16 Jul 2022 23:23:15 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/ros/</guid>
      <description>Robot Operating System / ROS Their docs has everything you need to refer, so we will just discuss ROS a bit here.
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is AccelerationRobotics. I quote their webpage:
ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient.</description>
      <content>&lt;h1 id=&#34;robot-operating-system--ros&#34;&gt;Robot Operating System / ROS&lt;/h1&gt;
&lt;p&gt;Their &lt;a href=&#34;https://docs.ros.org/&#34;&gt;docs&lt;/a&gt; has everything you need to refer, so we will just discuss ROS a bit here.&lt;br&gt;
It is one of the core knowledge and it is fundamental that there is a significant effort around it, one of the latest development, IMO, is &lt;a href=&#34;https://accelerationrobotics.com/robotcore.php#benchmarks&#34;&gt;AccelerationRobotics&lt;/a&gt;. I quote their webpage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ROBOTCORE™ helps build custom compute architectures for robots, or robot cores, that make robots faster, more deterministic and power-efficient. Simply put, it provides a development, build and deployment experience for creating robot hardware and hardware accelerators similar to the standard, non-accelerated ROS development flow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is &lt;a href=&#34;https://arxiv.org/abs/2205.03929&#34;&gt;RobotCore paper&lt;/a&gt; for reference.&lt;/p&gt;
&lt;h1 id=&#34;using-unity-with-ros&#34;&gt;Using Unity with ROS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Endpoint&#34;&gt;ROS TCP Endpoint&lt;/a&gt;&lt;br&gt;
&lt;em&gt;ROS package used to create an endpoint to accept ROS messages sent from a Unity scene using the ROS TCP Connector scripts&lt;/em&gt;\&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Related:&lt;/strong&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/ROS-TCP-Connector&#34;&gt;ROS TCP Connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/Unity-Robotics-Hub&#34;&gt;Robotics simulation using Unity&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Unity-Technologies/com.unity.perception&#34;&gt;Unity perception package&lt;/a&gt;&lt;br&gt;
&lt;em&gt;The Perception package provides a toolkit for generating large-scale datasets for computer vision training and validation. It is focused on a handful of camera-based use cases for now and will ultimately expand to other forms of sensors and machine learning tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Fake Real_Self_Awareness</title>
      <link>https://66mhz.github.io/robotics/fake-real_self_awareness/</link>
      <pubDate>Sat, 16 Jul 2022 23:15:47 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/fake-real_self_awareness/</guid>
      <description></description>
      <content></content>
    </item>
    
    <item>
      <title>Agriculture and Robotics</title>
      <link>https://66mhz.github.io/robotics/agriculture_robotics/</link>
      <pubDate>Sat, 16 Jul 2022 22:22:50 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/agriculture_robotics/</guid>
      <description>Survey TerraSentia (https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html) Functions Measure plant height Stem diameter Leaf-area index Stand count (num of live grain or fruit producing plants) Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.
In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.</description>
      <content>&lt;h1 id=&#34;survey&#34;&gt;Survey&lt;/h1&gt;
&lt;h2 id=&#34;terrasentia&#34;&gt;TerraSentia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&#34;&gt;https://www.nytimes.com/2020/02/13/science/farm-agriculture-robots.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;functions&#34;&gt;Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Measure plant height&lt;/li&gt;
&lt;li&gt;Stem diameter&lt;/li&gt;
&lt;li&gt;Leaf-area index&lt;/li&gt;
&lt;li&gt;Stand count (num of live grain or fruit producing plants)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dr.Chowdhary is working on this, and with its company EarthSense, which produces these robots.&lt;/p&gt;
&lt;p&gt;In addition to plant height, TerraSentia can measure stem diameter, leaf-area index and “stand count” — the number of live grain- or fruit-producing plants — or all of those traits at once. And Dr. Chowdhary is working on adding even more traits, or phenotypes, to the list with the help of colleagues at EarthSense, a spinoff company that he created to manufacture more robots.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>RaspberryPi Camera v2</title>
      <link>https://66mhz.github.io/robotics/raspberrycam/</link>
      <pubDate>Sat, 16 Jul 2022 22:10:03 -0500</pubDate>
      
      <guid>https://66mhz.github.io/robotics/raspberrycam/</guid>
      <description>Raspberry camera version 2.1 Sony IMX219 8-megapixel sensor Sensor: /base/soc/i2c0mux/i2c@1/imx219@10 It can be accessed through the MMAL and V4L APIs. If raspistill command does not work, check libcamera.
To install v4l2-rtspserver, simply use the following command:
sudo snap install v4l2-rtspserver
To check the status of uv4l on raspbian:
sudo service uv4l_raspicam status
To start a stream:
uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on
To find supported video capture formats:</description>
      <content>&lt;ul&gt;
&lt;li&gt;Raspberry camera version 2.1&lt;/li&gt;
&lt;li&gt;Sony IMX219 8-megapixel sensor&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sensor: /base/soc/i2c0mux/i2c@1/imx219@10&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;It can be accessed through the MMAL and V4L APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;code&gt;raspistill&lt;/code&gt; command does not work, check &lt;code&gt;libcamera&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To install &lt;em&gt;v4l2-rtspserver&lt;/em&gt;, simply use the following command:&lt;br&gt;
&lt;code&gt;sudo snap install v4l2-rtspserver&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check the status of uv4l on raspbian:&lt;br&gt;
&lt;code&gt;sudo service uv4l_raspicam status&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To start a stream:&lt;br&gt;
&lt;code&gt;uv4l --driver raspicam --auto-video_nr --encoding h264 --width 640 --height 480 --enable-server on&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To find supported video capture formats:&lt;br&gt;
&lt;code&gt;v4l2-ctl --list-formats&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To check supported and detected cameras:&lt;br&gt;
&lt;code&gt;vcgencmd get_camera&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How to use uv4l with a regular USB camera?&lt;br&gt;
&lt;code&gt;uv4l --driver uvc --auto-video_nr --device-path 001:003&lt;/code&gt;
Note &lt;code&gt;001:003&lt;/code&gt; shows the bus location for that USB camera, and we use &lt;code&gt;uvc&lt;/code&gt; for &lt;code&gt;--driver&lt;/code&gt;. This is the module for devices based on the USB video class definition.&lt;/p&gt;
&lt;p&gt;How to initiate stream with uv4l with a regular USB camera using uvc driver?&lt;br&gt;
&lt;code&gt;uv4l --device-path 001:003 --config-file=/etc/uv4l/uv4l-uvc.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;feeding-the-stream-to-slam-and-other-algorithms---disucssion&#34;&gt;Feeding the stream to SLAM and other algorithms - Disucssion&lt;/h3&gt;
&lt;p&gt;One of the approaches we can think of is using &lt;strong&gt;Adabins&lt;/strong&gt; and alike algorithms. These are the algorithms providing you depth information from a single image. The paper itself is quite a good read and &lt;a href=&#34;https://github.com/shariqfarooq123/AdaBins&#34;&gt;the code is here&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
